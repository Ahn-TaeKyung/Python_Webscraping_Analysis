# streamlit run streamlit_submit.py
import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import json

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‚¬ëŒì¸ ì±„ìš© ì •ë³´ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# JSON íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_json_data():
    file_path = 'data/'
    with open(file_path + 'PREFIX.json', 'r', encoding='utf-8') as f:
        PREFIX = json.load(f)
    with open(file_path + 'SUFFIX.json', 'r', encoding='utf-8') as f:
        SUFFIX = json.load(f)
    return PREFIX, SUFFIX

# ì§€ì—­ë³„ ìš”ì²­ íŒŒë¼ë¯¸í„° íŒŒì‹± í•¨ìˆ˜
@st.cache_data
def parse_data(PREFIX, SUFFIX):
    req_parameter = {
        district: PREFIX[region] + suffix[-3:]
        for region, districts in SUFFIX.items()
        for district, suffix in districts.items()
    }
    for region, districts in SUFFIX.items():
        if 'ì „ì²´' in districts:
            req_parameter[region] = PREFIX[region] + districts['ì „ì²´'][-3:]

    req_parameter2 = {
        'ì§€ì—­ë³„': 'domestic',
        'ì§ì—…ë³„': 'job-category',
        'ì—­ì„¸ê¶Œë³„': 'subway',
        'HOT100': 'hot100',
        'í—¤ë“œí—ŒíŒ…': 'headhunting'
    }
    return req_parameter, req_parameter2

def parse_location_input(user_input):

    # 3. ì§€ì—­ëª…ì„ ì½”ë“œë¡œ ë³€í™˜
    region_codes = []
    for region in user_input:
        code = req_parameter.get(region)
        if code:
            region_codes.append(code)
        else:
            print(f"ê²½ê³ : '{region}' ì§€ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return "%2C".join(region_codes)

# ì±„ìš© ì •ë³´ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_jobs(region_type, page_index, region_code):
    results = []
    if region_code == None:
        loc_cd_param = 101000
    else:
        loc_cd_param = parse_location_input(region_code)
    for page in range(1, page_index):
        url = (
            f"https://www.saramin.co.kr/zf_user/jobs/list/{region_type}"
            f"?page={page}&loc_cd={loc_cd_param}&search_done=y&preview=y"
        )
        headers = {'user-agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers)
        if not res.ok:
            st.error(f"ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
            continue

        soup = BeautifulSoup(res.text, 'html.parser')
        job_items = soup.select("div#default_list_wrap div.list_body div.box_item")

        for item in job_items:
            def extract_text(selector, default='Null'):
                tag = item.select_one(selector)
                return tag.text.strip() if tag else default

            def extract_href(selector, prefix="https://www.saramin.co.kr"):
                tag = item.select_one(selector)
                return prefix + tag['href'] if tag and 'href' in tag.attrs else 'NULL'

            results.append({
                'ë§í¬': extract_href("div.job_tit a.str_tit"),
                'íšŒì‚¬ë§í¬': extract_href("div.col.company_nm a.str_tit"),
                'ì œëª©': extract_text("div.job_tit span"),
                'íšŒì‚¬': extract_text("div.col.company_nm .str_tit"),
                'ì§ë¬´': ' / '.join(
                    span.get_text(strip=True)
                    for span in item.select(".job_sector span")
                ) or 'Null',
                'ì§€ì—­': extract_text(".recruit_info .work_place").rstrip(' ì™¸'),
                'ìš”êµ¬ê²½ë ¥': extract_text(".recruit_info .career"),
                'ìµœì†Œí•™ë ¥': extract_text(".recruit_info .education"),
                'ê¸°ê°„': extract_text("span.date"),
                'ë“±ë¡ì¼ì': extract_text("span.deadlines"),
                'ë°°ì§€': extract_text(".job_badge span"),
            })

    return pd.DataFrame(results)

def split_info(text):
    if not isinstance(text, str):
        return pd.Series([None, None])
    
    parts = re.split(r'\s*Â·\s*', text)

    if len(parts) >= 2:
        exp = ' Â· '.join(parts[:-1]).strip()
        job_type = re.sub(r'\s*ì™¸$', '', parts[-1].strip())
        return pd.Series([exp, job_type])
    else:
        return pd.Series([text.strip(), None])
    
def normalize_experience(exp):
    if not isinstance(exp, str):
        return None  # ë˜ëŠ” 'ê¸°íƒ€', 'ë¶ˆëª…' ë“±

    exp = exp.strip()
    if 'ì‹ ì…' in exp and 'ê²½ë ¥' in exp:
        return 'ì‹ ì…/ê²½ë ¥'
    elif 'ì‹ ì…' in exp:
        return 'ì‹ ì…'
    elif 'ê²½ë ¥ë¬´ê´€' in exp or 'ë…„ìˆ˜ë¬´ê´€' in exp:
        return 'ê²½ë ¥ë¬´ê´€'
    elif re.match(r'ê²½ë ¥ \d+ë…„â†‘', exp):
        years = re.findall(r'\d+', exp)[0]
        return f'{years}ë…„ ì´ìƒ'
    elif re.match(r'ê²½ë ¥ \d+ë…„â†“', exp):
        years = re.findall(r'\d+', exp)[0]
        return f'{years}ë…„ ì´í•˜'
    elif re.match(r'\d+ ~ \d+ë…„', exp):
        return exp.replace(' ', '')
    elif exp == 'ê²½ë ¥':
        return 'ê²½ë ¥'
    else:
        return exp
    
def process_registration_date(text):
    if not isinstance(text, str):
        return None
    
    # 'ìˆ˜ì •'ê³¼ 'ë“±ë¡' ì œê±°
    text = re.sub(r'\s*(ìˆ˜ì •|ë“±ë¡)\s*', '', text)
    
    # 'nì‹œê°„ ì „'ì„ 'ë‹¹ì¼'ë¡œ ë³€ê²½
    text = re.sub(r'\d+\s*ì‹œê°„\s*ì „', 'ë‹¹ì¼', text)
    
    # 'në¶„ ì „'ì€ ê·¸ëŒ€ë¡œ ë‘ê¸°
    text = re.sub(r'\d+\s*ë¶„\s*ì „', 'ë‹¹ì¼', text)
    
    return text.strip()

# ë©”ì¸ ì•±

def main():
    st.title(":mag: ì‚¬ëŒì¸ ì±„ìš© ì •ë³´ í¬ë¡¤ëŸ¬")
    st.markdown("""
        ì‚¬ëŒì¸ ì‚¬ì´íŠ¸ì—ì„œ ì›í•˜ëŠ” ì§€ì—­ì˜ ì±„ìš© ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.  
        ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì§€ì—­ ìœ í˜•ê³¼ ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”.
    """)

    PREFIX, SUFFIX = load_json_data()
    global req_parameter
    global req_parameter2
    req_parameter, req_parameter2 = parse_data(PREFIX, SUFFIX)

    with st.sidebar:
        st.header("ê²€ìƒ‰ ì„¤ì •")
        selected_region_type = st.selectbox("ì§€ì—­ ìœ í˜• ì„ íƒ", list(req_parameter2.keys()), index=0)
        region_type_code = req_parameter2[selected_region_type]

        selected_regions = None
        region_code = ''
        if selected_region_type == 'ì§€ì—­ë³„':
            regions = sorted(req_parameter.keys())
            selected_regions = st.multiselect("ì§€ì—­ ì„ íƒ", regions, default=['ì„œìš¸'] if 'ì„œìš¸' in regions else [])
            # region_code = [req_parameter.get(region, '') for region in selected_regions]

        search_button = st.button("ì±„ìš© ì •ë³´ ê°€ì ¸ì˜¤ê¸°", type="primary")

    if search_button:
        with st.spinner(f"{selected_regions or selected_region_type} ì±„ìš© ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            df = crawl_jobs(region_type_code, 10, selected_regions)

            # ë°ì´í„° ì „ì²˜ë¦¬
            df['ì›ë¬¸'] = df['ìš”êµ¬ê²½ë ¥']
            df[['ìš”êµ¬ê²½ë ¥_raw', 'ê³„ì•½ì¢…ë¥˜']] = df['ì›ë¬¸'].apply(split_info)
            # ì ìš©
            df['ìš”êµ¬ê²½ë ¥'] = df['ìš”êµ¬ê²½ë ¥_raw'].apply(normalize_experience)
            df.drop(columns=['ìš”êµ¬ê²½ë ¥_raw'], inplace=True)
            df['ë“±ë¡ì¼ì'] = df['ë“±ë¡ì¼ì'].apply(process_registration_date)
            df.dropna(axis = 0, inplace=True)

            if not df.empty:
                st.success(f"ì´ {len(df)}ê°œì˜ ì±„ìš© ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                st.dataframe(df, use_container_width=True, hide_index=True, column_config={
                    "ë§í¬": st.column_config.LinkColumn("ë§í¬", help="ì±„ìš© ê³µê³  í˜ì´ì§€"),
                })

                csv = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="CSVë¡œ ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name=f'saramin_jobs_{selected_regions or selected_region_type}.csv',
                    mime='text/csv'
                )
            else:
                st.warning("í•´ë‹¹ ì§€ì—­ì˜ ì±„ìš© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

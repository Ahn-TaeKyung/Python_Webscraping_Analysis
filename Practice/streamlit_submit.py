# streamlit run streamlit_submit.py
import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import json

st.set_page_config(
    page_title="ì‚¬ëŒì¸ ì±„ìš© ì •ë³´ í¬ë¡¤ëŸ¬",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_data
def load_json_data():
    file_path = 'data/'
    with open(file_path + 'PREFIX.json', 'r', encoding='utf-8') as f:
        PREFIX = json.load(f)
    with open(file_path + 'SUFFIX.json', 'r', encoding='utf-8') as f:
        SUFFIX = json.load(f)
    return PREFIX, SUFFIX

@st.cache_data
def parse_data(PREFIX, SUFFIX):
    req_parameter = {}
    region_subregion_map = {}

    for region, subregions in SUFFIX.items():
        region_code_prefix = PREFIX.get(region)
        if not region_code_prefix:
            continue

        region_subregion_map[region] = list(subregions.keys())
        for subregion, suffix_code in subregions.items():
            key = f"{region} {subregion}"
            req_parameter[key] = region_code_prefix + suffix_code

    return req_parameter, region_subregion_map

def parse_location_input(selected_region, selected_subregions):
    region_codes = []

    for district in selected_subregions:
        key = f"{selected_region} {district}"
        code = req_parameter.get(key)

        if code:
            region_codes.append(code)
        else:
            st.warning(f"âš ï¸ ì§€ì—­ ì½”ë“œ ì—†ìŒ: '{key}'")
    
    return "%2C".join(region_codes)

def split_info(text):
    if not isinstance(text, str):
        return pd.Series([None, None])
    parts = re.split(r'\s*Â·\s*', text)
    if len(parts) >= 2:
        exp = ' Â· '.join(parts[:-1]).strip()
        job_type = re.sub(r'\s*ì™¸$', '', parts[-1].strip())
        return pd.Series([exp, job_type])
    else:
        return pd.Series([text.strip(), None])

def normalize_experience(exp):
    if not isinstance(exp, str):
        return None
    exp = exp.strip()
    if 'ì‹ ì…' in exp and 'ê²½ë ¥' in exp:
        return 'ì‹ ì…/ê²½ë ¥'
    elif 'ì‹ ì…' in exp:
        return 'ì‹ ì…'
    elif 'ê²½ë ¥ë¬´ê´€' in exp or 'ë…„ìˆ˜ë¬´ê´€' in exp:
        return 'ê²½ë ¥ë¬´ê´€'
    elif re.match(r'ê²½ë ¥ \d+ë…„â†‘', exp):
        years = re.findall(r'\d+', exp)[0]
        return f'{years}ë…„ ì´ìƒ'
    elif re.match(r'ê²½ë ¥ \d+ë…„â†“', exp):
        years = re.findall(r'\d+', exp)[0]
        return f'{years}ë…„ ì´í•˜'
    elif re.match(r'\d+ ~ \d+ë…„', exp):
        return exp.replace(' ', '')
    elif exp == 'ê²½ë ¥':
        return 'ê²½ë ¥'
    else:
        return exp

def normalize_education(education):
    if not isinstance(education, str):
        return None
    education = education.strip()
    if 'â†‘' in education:
        education = education.replace('â†‘', '')
    return education

def process_registration_date(text):
    if not isinstance(text, str):
        return None
    text = re.sub(r'\s*(ìˆ˜ì •|ë“±ë¡)\s*', '', text)
    text = re.sub(r'\d+\s*ì‹œê°„\s*ì „', 'ë‹¹ì¼', text)
    text = re.sub(r'\d+\s*ë¶„\s*ì „', 'ë‹¹ì¼', text)
    return text.strip()

def extract_job_sectors(item):
    sector_tags = item.select(".job_sector span")
    sectors = [tag.text.strip() for tag in sector_tags if "ì™¸" not in tag.text]
    return sectors

def crawl_jobs(page_index, selected_region, selected_subregions):
    results = []
    loc_cd_param = parse_location_input(selected_region, selected_subregions)
    for page in range(1, page_index + 1):
        url = (
            f"https://www.saramin.co.kr/zf_user/jobs/list/domestic"
            f"?page={page}&loc_cd={loc_cd_param}&search_done=y&preview=y"
        )
        headers = {'user-agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers)
        if not res.ok:
            st.error(f"ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
            continue

        soup = BeautifulSoup(res.text, 'html.parser')
        job_items = soup.select("div#default_list_wrap div.list_body div.box_item")

        for item in job_items:
            def extract_text(selector, default='Null'):
                tag = item.select_one(selector)
                return tag.text.strip() if tag else default

            def extract_href(selector, prefix="https://www.saramin.co.kr"):
                tag = item.select_one(selector)
                return prefix + tag['href'] if tag and 'href' in tag.attrs else 'NULL'

            results.append({
                'ë§í¬': extract_href("div.job_tit a.str_tit"),
                'íšŒì‚¬ë§í¬': extract_href("div.col.company_nm a.str_tit"),
                'ì œëª©': extract_text("div.job_tit span"),
                'íšŒì‚¬': extract_text("div.col.company_nm .str_tit"),
                'ì§ë¬´': extract_job_sectors(item),
                'ì§€ì—­': extract_text(".recruit_info .work_place").rstrip(' ì™¸'),
                'ìš”êµ¬ê²½ë ¥': extract_text(".recruit_info .career"),
                'ìµœì†Œí•™ë ¥': extract_text(".recruit_info .education"),
                'ê¸°ê°„': extract_text("span.date"),
                'ë“±ë¡ì¼ì': extract_text("span.deadlines"),
                'ë°°ì§€': extract_text(".job_badge span"),
            })

    df = pd.DataFrame(results)
    print(df['ì§ë¬´'])
    if not df.empty:
        df[['ìš”êµ¬ê²½ë ¥_raw', 'ê³„ì•½ì¢…ë¥˜']] = df['ìš”êµ¬ê²½ë ¥'].apply(split_info)
        df['ìš”êµ¬ê²½ë ¥'] = df['ìš”êµ¬ê²½ë ¥_raw'].apply(normalize_experience)
        df['ìµœì†Œí•™ë ¥'] = df['ìµœì†Œí•™ë ¥'].apply(normalize_education)
        df['ë“±ë¡ì¼ì'] = df['ë“±ë¡ì¼ì'].apply(process_registration_date)
        df.drop(columns=['ìš”êµ¬ê²½ë ¥_raw'], inplace=True)
        df.dropna(axis=0, inplace=True)
        # df['ì§ë¬´'] = df['ì§ë¬´'].apply(lambda x: [job.strip() for job in x.split('/')])
    return df

# âœ… ê²½ë ¥ í•„í„°ë§ í•¨ìˆ˜
def filter_by_experience(df, exp_input):
    def parse_year_range(exp_text):
        if 'ì‹ ì…' in exp_text or 'ê²½ë ¥ë¬´ê´€' in exp_text:
            return 0, 0
        elif 'ë…„ ì´ìƒ' in exp_text:
            year = int(re.findall(r'\d+', exp_text)[0])
            return year, float('inf')
        elif 'ë…„ ì´í•˜' in exp_text:
            year = int(re.findall(r'\d+', exp_text)[0])
            return 0, year
        elif re.match(r'\d+~\d+ë…„', exp_text):
            nums = list(map(int, re.findall(r'\d+', exp_text)))
            return nums[0], nums[1]
        return None, None

    if exp_input == 'ì‹ ì…':
        return df[df['ìš”êµ¬ê²½ë ¥'].isin(['ì‹ ì…', 'ê²½ë ¥ë¬´ê´€', 'ì‹ ì…/ê²½ë ¥'])]

    try:
        exp_val = int(exp_input)
    except ValueError:
        return df

    filtered_rows = []
    for _, row in df.iterrows():
        exp = row['ìš”êµ¬ê²½ë ¥']
        min_exp, max_exp = parse_year_range(exp)
        if min_exp is None:
            continue
        if min_exp <= exp_val <= max_exp:
            filtered_rows.append(row)

    return pd.DataFrame(filtered_rows)

def main():
    st.title(":mag: ì‚¬ëŒì¸ ì±„ìš© ì •ë³´ í¬ë¡¤ëŸ¬")
    st.markdown("""ì‚¬ëŒì¸ ì‚¬ì´íŠ¸ì—ì„œ ì›í•˜ëŠ” ì§€ì—­ì˜ ì±„ìš© ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.""")

    PREFIX, SUFFIX = load_json_data()
    global req_parameter
    global region_subregion_map
    req_parameter, region_subregion_map = parse_data(PREFIX, SUFFIX)

    if "all_jobs" not in st.session_state:
        st.session_state.all_jobs = pd.DataFrame()
    if "filtered_jobs" not in st.session_state:
        st.session_state.filtered_jobs = pd.DataFrame()

    with st.sidebar:
        st.header("ëŒ€ë¶„ë¥˜ ì§€ì—­ ì„ íƒ")
        selected_region = st.selectbox("ëŒ€ë¶„ë¥˜ ì§€ì—­", list(region_subregion_map.keys()))

        st.header("ì„¸ë¶€ ì§€ì—­ ì„ íƒ")
        subregions = region_subregion_map[selected_region]
        selected_subregions = st.multiselect("ì„¸ë¶€ ì§€ì—­", subregions)

        if st.button("ê³µê³  ê°€ì ¸ì˜¤ê¸°"):
            with st.spinner("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                st.session_state.all_jobs = crawl_jobs(10, selected_region, selected_subregions)
                st.session_state.filtered_jobs = st.session_state.all_jobs.copy()
                st.success(f"{len(st.session_state.all_jobs)}ê°œì˜ ê³µê³ ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!")

        if not st.session_state.all_jobs.empty:
            st.header("í•„í„°")
            unique_jobs = sorted(set(
                job for job_list in st.session_state.all_jobs['ì§ë¬´'].dropna() if isinstance(job_list, list) for job in job_list
            ))
            selected_jobs = st.multiselect("ì§ë¬´ ì„ íƒ", unique_jobs)

            filtered_df = st.session_state.all_jobs.copy()
            if selected_jobs:
                filtered_df = filtered_df[filtered_df['ì§ë¬´'].apply(lambda x: any(job in x for job in selected_jobs))]

            # âœ… ê²½ë ¥ í•„í„° ì…ë ¥ UI
            st.header("ìš”êµ¬ ê²½ë ¥ í•„í„°")
            experience_input = st.text_input("ê²½ë ¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì‹ ì… ë˜ëŠ” ìˆ«ì)", "")

            if experience_input:
                filtered_df = filter_by_experience(filtered_df, experience_input)

            st.session_state.filtered_jobs = filtered_df

    if not st.session_state.filtered_jobs.empty:
        st.header("ê²€ìƒ‰ ê²°ê³¼")

        if st.button("ìƒˆë¡œê³ ì¹¨"):
            st.session_state.all_jobs = pd.DataFrame()
            st.session_state.filtered_jobs = pd.DataFrame()
            st.experimental_rerun()

        st.dataframe(st.session_state.filtered_jobs, use_container_width=True, hide_index=True)

        csv = st.session_state.filtered_jobs.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="CSVë¡œ ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name="filtered_jobs.csv",
            mime="text/csv"
        )
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
